library(ggplot2)
library(latticeExtra)
library(gridExtra)
p_grid <-seq(0,1,length.out=1000)
prior<- rep(1,1000)
likelihood<-dbinom(6,9,prob=p_grid)
posterior<-likelihood*prior
posterior<-posterior/sum(posterior)
samples<-sample(p_grid,1e4,prob=posterior,replace=T)
plot(posterior)
loss<-sapply(p_grid,function(d) sum(posterior*abs(d-p_grid)))
lines(loss/1000,col='red')
p_grid[which.min(loss)]
median(samples)
mean(samples)
length(samples)
############
n=5
p_grid <-seq(0,1,length.out=n)
prior<- rep(1,n)
likelihood<-dbinom(6,9,prob=p_grid)
posterior<-likelihood*prior
posterior<-posterior/sum(posterior)
unst.posterior<-likelihood*prior
Calculate_loss<-function(d) {
for (i in 1:length(d)) {
return((abs(d[i]-p_grid)))   #this creates a matrix of p_grid differences; diagnol == zero
}
}
loss<-sapply(p_grid,Calculate_loss) #this creates The matrix
unst.posterior   #for output comp
loss  #for output comp
posterior*loss  #notice that the vector posterior multiplies column by col, not by row, so transpose in head,
#the following code basically plots the absolute deviation of guesses from possible outcomes
wireframe(loss,drape=T,ylab = list('Y axis\nYour guesses\ncolumns 1\n through n\n\n',rot=0),xlab = list('X axis\npossible \ntrue values \n\n',rot=0),main="difference of guess from reality",screen = list(z =-90 , x = -70, y=0))
#but we need to start adding back the components of the function, starting with finding the absolute differences because we are really only interested in magnitude of loss, not the direction (how do you have negative loss?)
loss.1<-apply(loss,2,sum) # collapses grid into vector, the final desired output
loss.2<-sapply(p_grid,function(d) sum(posterior*abs(d-p_grid))) #original formula from book
plot(posterior,x=p_grid,type='b',col='blue',main = paste("length.out  = ",length(p_grid)))
lines(loss.2/10,x=p_grid,type='b',col='red')
abline(v=p_grid[which.min(loss.2)],col='red')
graph_text<-paste("low P_grid",round(p_grid[which.min(loss.2)],3),"----->")
legend ("topleft",paste("Loss minimized\nat P_grid =",round(p_grid[which.min(loss.2)],3)))
df<-data_frame(p_grid,posterior)
p<-ggplot(df, aes(p_grid,posterior))
P<-p+geom_line()+geom_line(aes(y=loss.2/10),color='red')+geom_rug(aes(x=p_grid,y=loss.2/10))
par(mfrow=c(1,2))
p<-ggplot(df, aes(p_grid,posterior))
p+geom_line()+geom_line(aes(y=loss.2/10),color='red')+geom_rug(aes(x=p_grid,y=loss.2/10))
par(mfrow=c(1,2))
loss.df<-as_data_frame(loss)
colnames(loss.df)<-paste0("diff",seq(1,n))
tidy.loss.df<-loss.df %>% gather(difference,value)
tidy.loss.df<-cbind(p_grid,tidy.loss.df)
wf<-wireframe(posterior*loss,
drape=T,
main="loss function before \ncolapsed by sum",
#light.source = c(0,10,10),
screen = list(z =-105 , x = -70, y=0),
xlab = list("possible\n probabilities",rot=0),
ylab = list('Your guesses',rot=0),
zlab = list("differences between\n guess and actual",rot=90),
zoom=.9,
scales = list(arrows = FALSE,
x=list(draw=F),
y=list(draw=F),
z=list(draw=F),
#col = "black",
# font = 1,
#tck = c(0.8, 0.6, 0.4),
distance =c(1.2,.8, 1.45)),
col.regions = colorRampPalette(c("blue", "red"))(100)
)
loss.p<-xyplot(loss.2/10~p_grid,col='red')
lattice.options(
layout.heights=list(bottom.padding=list(x=0), top.padding=list(x=0)),
layout.widths=list(left.padding=list(x=0), right.padding=list(x=0))
)
grid.arrange(wf, P, ncol=2, top = "Loss function over\n Posterior Distribution")
cloud(posterior*loss,
panel.3d.cloud=panel.3dbars,
col.facet='grey',
xbase=0.4,
ybase=0.4,
#      scales=list(arrows=FALSE, col=1),
screen = list(z = 48, x = -90, y=0),
xlab = "possible probabilities",
ylab = 'Your guesses',
zlab = "differences",
par.settings = list(axis.line = list(col = "transparent"))
)
print(wf,split=c(1,1,1,1),more=T)
print(loss.p,split=c(1,1,1,1),more=T)
knitr::opts_chunk$set(echo = TRUE)
x<-c('dplyr','tidyr','ggplot2','lubridate','kableExtra','knitr')
lapply(x, require, character.only = TRUE)
coin_toss<-function(sample_size=3,trial_length=10, p=.5){
tally_df<-matrix(ncol=4,nrow=sample_size)
tally_df<-as.data.frame(tally_df)
names(tally_df)<-c('heads','tails','difference','average')
for(i in 1:sample_size){
#trial<-rbinom(trial_length,1,p)
trial<-sample(0:1,10,.5)
#print(trial)
average<-mean(trial)
h<-sum(trial)
t<-trial_length-h
diff<-(h-t)
#print(i)
#print(h)
#print(t)
tally_df[i,]<-(cbind(h,t,diff,average))
}
return(tally_df)
}
tally_df<-coin_toss(sample_size=10,trial_length=10,p=.5)
tally_df
head(tally_df)
##assuming a probability that the coin is fair, (.5), here is the average difference of all those coin differences from that average.
mean(tally_df[,3])
plot(density(tally_df[,3]))
lines(density(tally_df[,3]),col='blue')
simulate_coin_toss<-function(sims=5,
sample_size=3,
trial_length=10,
heads=3,
p=.5){
#pvalue_df<-vector(length=sims)
pvalue_df<-as.data.frame(matrix(nrow=sims,ncol=2))
names(pvalue_df)<-c('pvalue','success_count')
print (' here is an empty data set to store a pvalue and the number of success')
head(pvalue_df,2)
hist(rbinom(10,10,.5),breaks=20,
main=paste('count of heads in 10 in 10 flips'),
xlab='count of heads')
for (j in 1:sims){
tally_df<-coin_toss(sample_size = sample_size,trial_length = trial_length, p)
#print(tally_df[j,])
#cat(paste('\r\nnum of rows in tally_df: ',nrow(tally_df)," \r\n"))
#cat(paste('\r\nnum of times we found ',
#         tally_df$heads[tally_df$heads<=heads],
#        " heads or fewer \r\n\r\n"))
pvalue_df[j,1]<-length(tally_df$heads[tally_df$heads<=heads])/nrow(tally_df)
pvalue_df[j,2]<-length(tally_df$heads[tally_df$heads<=heads])
#abline(v=pvalue_df[j,2],col='blue')
#print(pvalue_df[j,1:2])
}
head(pvalue_df)
return(pvalue_df)
}
heads=3
simulate_coin_toss(sims=2,sample_size=1,trial_length=10,heads=heads)
pvalue_df<-simulate_coin_toss(sims=100,sample_size=10,trial_length=10,heads=heads)
mean(pvalue_df$pvalue) #calculates average cumulative probablity of a given number of heads/trials
sd(pvalue_df$pvalue)
hist(pvalue_df$pvalue,freq=F,main="distribution of p-values",xlab='pvalues')
text(x=.3,y= 5,labels = 'blue line is simulated pvalue')
abline(v=mean(pvalue_df$pvalue),col='blue')
abline(v=pbinom(heads,10,.5),col='red',lty=3) #plots verticle line of exact cumulative  prob of heads/trials
1*(1-pbinom(17,25,.5)) #this computes one tailed test
binom.test(x=c(18,7),p=.5,alternative = ("l")) #here c(18,7) refelct success and failures, 'l' is less for one tailed
binom.test(x=3,n=10,p=.5,alternative = c("l")) #here x=3,n=10 reflect success and trials, 'l' is less for one tailed
library(dplyr)
exp<-list(5,5)
obs<-list(2,8)
dbinom(2,10,.5)  #this analytically calculates the probability
mean(rbinom(1000,10,.3)==2)
out<-as.integer()
for (i in 1:length(obs)){
out[i]<-(as.numeric(obs[i])-as.numeric(exp[i]))^2  #squaring the differences
#print(out[i])
}
thresh<-sum(out)  #sum of squares
knitr::opts_chunk$set(echo = TRUE)
plot(rnorm(100,mean=66, sd=1))
plot(density(rnorm(100,mean=66, sd=1)))
hist(rnorm(100,mean=66, sd=1))
h <- rnorm(100,mean=66, sd=1)
print(h)
h <- round(rnorm(100,mean=66, sd=1),2)
print(h)
hist(h,xlab = 'Height')
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
h <- round(rnorm(100,mean=66, sd=1),1)
print(h/12)
h <- round(rnorm(100,mean=66, sd=1),1)
print(round(h/12,1))
h <- round(rnorm(100,mean=66, sd=1)/12,1)
print(h)
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
h <- round(rnorm(100,mean=66, sd=1),1)
print(h)
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
abline(y=6)
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
abline(y=6)
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
abline(h=6)
sort(h)
h[h<64.5]
sort(h)
h[h<=64.5]
h[h>=64&h<=64.5]
sort(h)
sort(h[h>=64&h<=64.5])
sort(h[h>=64&h<=64.5])
sort(h)
print("and here is the data pulled out so you aren't overtaxing your eyes"")
sort(h[h>=64&h<=64.5])
sort(h)
print("and here is the data pulled out so you aren't overtaxing your eyes")
sort(h[h>=64&h<=64.5])
length(h[h>=64&h<=64.5])
h.1 <- rnorm(200, mean=66)
hist(h.1,xlab = 'Height',main = 'histogram of 200 heights in Seattle')
h.1 <- rnorm(200, mean=66)
hist(h.1,xlab = 'Height',main = 'histogram of 200 heights in Seattle')
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
curve(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
lines(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
lines(density(dnorm(h.1000)))
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
density(h.1000)
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
lines(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
lines(density(h.1000)*1000)
h.1000 <- rnorm(1000, mean=66)
par(mfrow=c(2,1))
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
par(mfrow=c(1,2))
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000))
h.1000 <- rnorm(1000, mean=66)
par(mfrow=c(1,2))
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000),xlab="heights",main="Distribution")
datafilename <- "http://personality-project.org/r/datasets/maps.mixx.epi.bfi.data"
person.data  <- read.table(datafilename,header=TRUE)  #read the data file
str(person.data)
cor(person.data$bfneur,person.data$epiNeur)
plot(person.data$bfneur,person.data$epiNeur)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
install.packages("ggplot2")
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(ggplot)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
#library(ggplot)
person.data %>% lm(epiNeur~bfneur)
lm(person.data,epiNeur~bfneur)
?lm
lm(data=person.data,epiNeur~bfneur)
plot(person.data$bfneur,person.data$epiNeur)
#abline()
Neurot.lm.1  <- lm(data=person.data,epiNeur~bfneur)
plot(person.data$bfneur,person.data$epiNeur)
abline(Neurot.lm.1)
Neurot.lm.1
summary(Neurot.lm.1)
person.data$bfneur-(mean(person.data$bfneur)/sd(person.data$bfneur))
person.data$epiNeur-(mean(person.data$epiNeur)/sd(person.data$epiNeur))
epiN.std <- person.data$epiNeur-(mean(person.data$epiNeur)/sd(person.data$epiNeur))
bfN.std <- person.data$bfneur-(mean(person.data$bfneur)/sd(person.data$bfneur))
cor(epiN.std,bfN.std)
convert_to_z <- function(x){
if(is.numeric(x)==F){
return(print('data must be of numeric type'))
}
else{
(x-mean(x))/sd(x)
}
}
mean(convert_to_z(1:10)) #should be zero
cor(convert_to_z(person.data$epiNeur),convert_to_z(person.data$bfneur))
sd(person.data$epiNeur)
sd(person.data$bfneur)
cor(convert_to_z(person.data$epiNeur),convert_to_z(person.data$bfneur))
.63*sd(person.data$epiNeur)/sd(person.data$bfneur)
cor(person.data$epiNeur,person.data$bfneur)
cor(person.data$epiNeur,person.data$bfneur)*sd(person.data$epiNeur)/sd(person.data$bfneur)
summary(Neurot.lm.1)
plot(person.data$bfneur,person.data$bfneur)
plot(person.data$bfneur,person.data$bfneur+rnorm(length(person.data$bfneur,.1,3)))
plot(person.data$bfneur,person.data$bfneur+rnorm(length(person.data$bfneur),.1,3)))
plot(person.data$bfneur,person.data$bfneur+rnorm(length(person.data$bfneur),.1,3))
plot(person.data$bfneur,person.data$bfneur+rnorm(length(person.data$bfneur),.1,3),ylab='noise added to bfneur')
plot(person.data$bfneur,person.data$bfneur+rnorm(length(person.data$bfneur),.1,3),ylab='noise added to bfneur')
bfneur.noise <- person.data$bfneur+rnorm(length(person.data$bfneur),.1,3)
plot(person.data$bfneur,bfneur.noise,ylab='noise added to bfneur')
summary(lm(bfneur.noise~person.data$bfneur))
bfneur.noise <- person.data$bfneur+rnorm(length(person.data$bfneur),.1,3)
plot(person.data$bfneur,bfneur.noise,ylab='noise added to bfneur')
summary(lm(person.data$bfneur~person.data$bfneur))
---
title: "Abnromal stats"
author: "Brian Holt"
date: "10/15/2019"
output:
html_document:
TOC: yes
df_print: paged
pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
#library(ggplot)
```
## Intro to stats for abnormal psych students
Big idea about research is that it is incredibly rare to perform one study/experiment and to have it change the known literature.  Science is 99.99% incremental.
So, when you learn about a topic via reading journal articles (experiments) be humble about what you find.
Also, remember the 3 main parts of John Stuart Mills thoughts on Cause and effect.  To establish that you need:
1. Covariance between variables (correlation)
2. Temporal precedence (the cause has to come before the effect
3. all other explanations must be ruled out.
The last one, 3, is the hard part.
This is a skeletal outline of most intro stat books:
What are numbers
How to graph
Probability
Probability Distribution broadly
Probability Distributions, specifically the Normal, central limit theorem
Z-scores
T-tests
Correlations
ANOVA
Linear Regression
Chi-Square
Effect Sizes
In abnormal psychology you see a lot of linear regressions.  You also see a Effect Sizes.
To have an introductory understanding of regression, you should understand probability and Probability Distributions.
So, let’s jump into probability distributions by first looking at some graphs.
##Graphing with a histogram
Histograms are useful for seeing how small sets of data "look".  It often is useful to do this for seeing where your data exists. Imagine we wanted to sample how tall people are.  Let us say I can sample 100 people, randomly, from Seattle.  Here they are listed in inches
```{r height data}
h <- round(rnorm(100,mean=66, sd=1),1)
print(h)
```
We can see this data more easily if we graph it.  There are many ways to, but htis is a class way: The Histogram
```{r histogram}
hist(h,xlab = 'Height',main = 'histogram of heights in Seattle')
```
What you should see here is that the vertical 'y' axis is a frequency, a count, while the 'x' axis is the range of scores.  So, from 64 to 64.5 inches tall, there appears to be about `r length(h[h>=64&h<=64.5])` people.  If you look at our original data, you should see this is true:
```{r sort data,echo=F}
sort(h)
print("and here is the data pulled out so you aren't overtaxing your eyes")
sort(h[h>=64&h<=64.5])
```
But what I want you to really see is the shape of this histogram.  It is beginning to look like a bell curve, the classic normal distribution.  If we could sample a bit larger number of people, say 200, you would a shape more close to this normal curve:
```{r More data for histogram,echo=F}
h.1 <- rnorm(200, mean=66)
hist(h.1,xlab = 'Height',main = 'histogram of 200 heights in Seattle')
```
And what about 1000 people?
```{r  1000 data for histogram,echo=F}
h.1000 <- rnorm(1000, mean=66)
par(mfrow=c(1,2))
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000),xlab="heights",main="Distribution")
```
I hope you see where this is going.  Because then the following makes some sense:
The area under the curve on the right is the probabilty of those values on the x axis occuring.
For example, look at the histogram of 100 people.  We can use the the number of people between 64 and 64.5 inches.  In this case there are `r length(h[h>=64&h<=64.5])`, and since there are 100 people, you can do the quick calcuation that `r length(h[h>=64&h<=64.5])` out of 100 is `r length(h[h>=64&h<=64.5])/100`.  If you were to ask the question "what is the chance that someone is between 64 and 64.5 inches tall, you would say about `r length(h[h>=64&h<=64.5])`%
The point here is that if you know something about a samples's distribution, you can start making some guesses about the larger population.  And, this is the cornerstone for what a p-value is.  But we'll come back to that.
##Big leap to correlations
I'm going to load a data set that has 231 cases, people, where they have given data about their personality.  I've no idea about who these people are, though I imgaine there is some information online.
```{r load data, echo=T}
datafilename <- "http://personality-project.org/r/datasets/maps.mixx.epi.bfi.data"
person.data  <- read.table(datafilename,header=TRUE)  #read the data file
str(person.data)
```
We've talked about neuroticism in the context of the Big 5 OCEAN but not of the PEN, a different trait theory about personality.  The N in PEN stands for neuroticism, and so let's see the correlation.
First, it helps to plot the data.  On the x-axis we'll put the big 5 Neuroticism and the y-axis will have the PEN.
```{r plot neuroticisms, echo=F}
plot(person.data$bfneur,person.data$epiNeur)
```
This plot shows the data leaning, yes?  This is visually describing a "positive" correlation:  when one varialbe moves, the other variable moves **in the same direction**.  If they moved in opposite directions, one goes, up the other down, you'd have a negative correlation.  If the two variables were unrelated, then the correlation would look much like a big scatter of data points with no obvious trend.
A correlation is a numerical representation of these trends.  It exists as a number between -1 --- 0 --- +1. For this data set, the correlation happens to be `r round(cor(person.data$bfneur,person.data$epiNeur),2)`
You can also visualize a correlation as the best fitting line of this data.  And this is *basically* what linear regression is.
## Linear Regression
You may remember in past math courses something about the best fitting line.  You more likely remember one of the formulas for a line is y=mx+b.
Well, a regression is sort of like the varialbe 'm'.  Let's plot this line over the data.
```{r linear regression line on data}
Neurot.lm.1  <- lm(data=person.data,epiNeur~bfneur)
plot(person.data$bfneur,person.data$epiNeur)
abline(Neurot.lm.1)
```
You should notice the line doesn't fit over every single data point.  What it is trying to do is mimize the overall difference scores from the line to each of the data points.  You can do this by hand with small data sets and when you are trying to do just 2 variables. More variables requires linear algebra, and the more varialbes you add, the time it takes to solve by hand is exponential. Like if you try to do 4 varibles it will take you weeks.
Yay for computers.  Cuz the fast.
What this line represents is the best line where the difference between data points and this line is the smallest.
It is implying a causal relationship, or at least that one variable 'X' impacts the other variable 'Y'.  Maybe it's causal, maybe there are other variables that are doing the 'causing'.
When you see a paper cite a linear regression the output will look something like this:
```{r linear regression output,echo=T}
summary(Neurot.lm.1)
```
The key number here is the "beta" Estimate for bfneur, 0.1318, which basically says that as x moves one unit, y will move .1318 units up.  Seems small, but in this case it is significant.  Imagine a scenario when you took the same variable and plotted it against itself:
```{r plot bfneur to itself,echo=T}
plot(person.data$bfneur,person.data$bfneur)
```
It's a straigt line.  Now let's add a little variation and run a regression to see how big the Beta coefficient gets.
```{r linear regress on bfneur, echo=T}
bfneur.noise <- person.data$bfneur+rnorm(length(person.data$bfneur),.1,3)
plot(person.data$bfneur,bfneur.noise,ylab='noise added to bfneur')
summary(lm(bfneur.noise~person.data$bfneur))
```
The summary output shows a 1.0 for the beta coefficient. 1 move for every move of 1.  Beta coefficients can be larger than 1 whereas correlations must be between -1 and +1.
```{create beta coefficient from cor,echo=TRUE}
cor.bfneur.epin <- cor(person.data$epiNeur,person.data$bfneur)
cor.bfneur.epin
sd(person.data$epiNeur)/sd(person.data$bfneur)
```
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
#library(ggplot)
cor.bfneur.epin <- cor(person.data$epiNeur,person.data$bfneur)
cor.bfneur.epin*sd(person.data$epiNeur)/sd(person.data$bfneur)
install.packages("ggplot2")
install.packages("nlme")
install.packages("nlme")
install.packages("mgcv")
install.packages("mgcv")
install.packages("nlme")
install.packages("nlme")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("tidyverse")
install.packages("stringi")
install.packages("tidyverse")
getwd()
iq<- c(61, 88, 89, 89, 90, 92, 93, 94, 98, 98, 101, 102, 105, 108, 109, 113, 114, 115, 120, 138)
sd(iq)
mean(iq)
median(iq)
sd(iq) sqrt(20)
sd(iq)/ sqrt(20)
sample(iq,30)
sample(30,iq)
sample(20)
sample(iq,20)
replicate(sample(iq,20),2)
replicate(sample(iq,20),length=2)
replicate(2,sample(iq,20))
boot<- replicate(ie5,sample(iq,20))
boot<- replicate(10000,sample(iq,20))
apply(boot,2,mean)
boot
boot<- replicate(10000,sample(iq,20,replace = T))
apply(boot,2,mean)
sd(boot)
sd(boot)/sqrt(10000)
sapply(boot,2,mean)
means<- apply(boot,2,mean)
sd(means)
sort(means)
means<- sort(means)
means
quantile(mean)
quantile(means)
?quantiles
?quantile
quantile(means, seq(.025,.975))
quantile(means, seq(.025))
quantile(means, seq(.975))
quantile(means, seq(0,.025,.975,1))
quantile(means, probs=seq(0,1,.025))
setwd("~/Projects/enrollment")
library(RSelenium)
library(rvest)
library(tidyverse)
driver <- rsDriver(browser =c("firefox"))
##https://github.com/ropensci/RSelenium/issues/116
remote_driver <- driver[["client"]]
remote_driver$open()
#remote_driver$close()
remote_driver$navigate("https://inside.seattlecolleges.edu/default.aspx?svc=enrollment&page=enrollment")
RSelenium:::selKeys %>% names()
remote_driver$refresh()
el<- remote_driver$findElement(using='xpath', '//*[@id="TxSID"]')
el$highlightElement()
el$clickElement()
el$sendKeysToElement(list('980354189'))
el.1 <- remote_driver$findElement(using = 'xpath', '//*[@id="TxPIN"]')
el.1$highlightElement()
el.1$sendKeysToElement(list('652383'))
el$sendKeysToElement(list(key='enter'))
el$sendKeysToElement(list(key='enter'))
remote_driver$close()
